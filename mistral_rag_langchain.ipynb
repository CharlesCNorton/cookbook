{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19969669-b47f-47f3-b6d4-f7b155434840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --quiet chromadb tiktoken openai langchain-community langchain-together langchain-mistralai langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d4c61-1d93-45d2-b6ca-8d32f18b0e06",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, we load the [Mixtral paper](https://arxiv.org/pdf/2401.04088.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f881e18-b175-4117-8441-d04c76fca375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2401.04088.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23902e3-47d1-4283-a338-0f980af1eb9b",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "Next, we split the based upon token count [using tiktoken encoder](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token#tiktoken).\n",
    "\n",
    "We use [recursive](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter) splitting: \n",
    "\n",
    "> This has the effect of trying to keep all paragraphs (and then sentences, and then words) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4457c040-d38a-4e8e-9fe8-fb1cc411be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=2000,chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34ea93-d6a0-4cae-ad15-037aefef4966",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "First, set up an account with Mistral to access their embedding model [here](https://console.mistral.ai/users/api-keys/).\n",
    "\n",
    "We'll embed [using Mistral](https://python.langchain.com/docs/integrations/text_embedding/mistralai) and index the chunks using a local vectorstore, [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9c78e9-d7b7-4d81-b832-d95c32b85b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# Try Mistral embd\n",
    "mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
    "# Try OpenAI embd\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0842752d-3ade-4c6f-b891-0858b0d0b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e39e0-039e-4c6d-bc08-2c1801e1e0d3",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Promt we'll use for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91184664-a786-4768-96fd-d77c7856f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f897c-1c21-41b8-86d6-1f1c34ea7145",
   "metadata": {},
   "source": [
    "## RAG chain\n",
    "\n",
    "Compose our retriever, prompt, and LLM.\n",
    "\n",
    "For LLM, we can using [Together](https://python.langchain.com/docs/integrations/llms/together) or [Mistral](https://python.langchain.com/docs/integrations/chat/mistralai) [API](https://docs.mistral.ai/api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "980c7b3f-f402-45d8-9a65-5d8c8f73e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "llm = ChatMistralAI(model=\"Mistral-7B-v0.2\",\n",
    "                    mistral_api_key=mistral_api_key)\n",
    "\n",
    "from langchain_community.llms import Together\n",
    "llm = Together(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=2000,\n",
    "    top_k=1,\n",
    ")\n",
    "\n",
    "# RAG chain\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \n",
    "                      \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84895265-6f67-442c-b234-0107108607c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The Mixtral bias on the BBQ benchmark is 56.0%.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the Mixtral bias on the BBQ benchmark?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d648c54-2f2b-4e57-80f9-dc2e8a852891",
   "metadata": {},
   "source": [
    "## Optional: Use LangSmith for tracing.\n",
    "\n",
    "Compose our retriever, prompt, and LLM.\n",
    "\n",
    "Set up LangSmith [as discussed here](https://python.langchain.com/docs/langsmith/walkthrough): \n",
    "\n",
    "* Trace: https://smith.langchain.com/public/6d3f07ef-25b8-4392-ba3a-d5ddfc26d980/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
